{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_JSON = \"../data/input/biorxiv_medrxiv/biorxiv_medrxiv/0015023cc06b5362d332b3baf348d11567ca2fbb.json\"\n",
    "TEST_JSON_2 = \"../data/input/biorxiv_medrxiv/biorxiv_medrxiv/00340eea543336d54adda18236424de6a5e91c9d.json\"\n",
    "test_json = json.load(open(TEST_JSON, 'r'))\n",
    "\n",
    "papers_list = [json.load(open(TEST_JSON, 'r')),\n",
    "               json.load(open(TEST_JSON_2, 'r')),\n",
    "               json.load(open(TEST_JSON, 'r')),\n",
    "               json.load(open(TEST_JSON_2, 'r'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red\">Knowledge Graph</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Code\n",
    "\n",
    "`!python -m spacy download en_core_web_lg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import functools\n",
    "from functools import reduce, partial\n",
    "import numpy as np\n",
    "import operator\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITY_INDICATORS = [\"NOUN\", \"PROPN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_dups(relations):\n",
    "    \"\"\"Removes duplicate triples from list.\"\"\"\n",
    "    return list(set(relations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_filter(text):\n",
    "    \"\"\"Removing numbers and puncutation.\"\"\"\n",
    "    text = re.sub(r\" \\d+\", \"\",text)\n",
    "    return re.sub(r\"[^A-Za-z0-9 -]+\", \"\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_name(author):\n",
    "    \"\"\"Combing first and last name of authors.\"\"\"\n",
    "    return author[\"first\"].lower() + \" \" + author[\"last\"].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_text(text):\n",
    "    \"\"\"Combing list of text elements into one.\"\"\"\n",
    "    return \" \".join(regex_filter(seg['text']) for seg in text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_bib_text(bib):\n",
    "    ref_iter = bib.values()\n",
    "    return \" \".join(regex_filter(ref['title']) for ref in ref_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(text):\n",
    "    \"\"\"Returns the entities from a spacy token sequence.\"\"\"\n",
    "    return [token.text.lower() for token in text if token.pos_ in ENTITY_INDICATORS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relation(nlp, paper):\n",
    "    \"\"\"\n",
    "    Read paper and collect triple relations.\n",
    "    \"\"\"\n",
    "    \n",
    "    triples = []\n",
    "    \n",
    "    # Paper ID\n",
    "    paperid = paper['paper_id']\n",
    "    \n",
    "    # Author\n",
    "    authors_list = list(map(construct_name, paper[\"metadata\"][\"authors\"]))\n",
    "    authors_triples = [(author, paperid, \"has_author\") for author in authors_list]\n",
    "    \n",
    "    # Title\n",
    "    title_entities = get_entities(nlp(paper['metadata']['title']))\n",
    "    title_triples = [(title_entity, paperid, \"has_title\") for title_entity in title_entities]\n",
    "    \n",
    "    # Abtract\n",
    "    abstract_text = collect_text(paper['abstract'])\n",
    "    abstract_entities = get_entities(nlp(abstract_text))\n",
    "    abstract_triples = [(abs_entity, paperid, \"has_abstract\") for abs_entity in abstract_entities]\n",
    "    \n",
    "    # Body\n",
    "    body_text = collect_text(paper['body_text'])\n",
    "    body_entities = get_entities(nlp(body_text))\n",
    "    body_triples = [(body_entity, paperid, \"has_body\") for body_entity in body_entities]\n",
    "\n",
    "    # Bibliography\n",
    "    bib_text = collect_bib_text(paper['bib_entries'])\n",
    "    bib_entities = get_entities(nlp(bib_text))\n",
    "    bib_triples = [(bib_entity, paperid, \"has_body\") for bib_entity in bib_entities]\n",
    "    \n",
    "    triples.extend(rm_dups(authors_triples))\n",
    "    triples.extend(rm_dups(title_triples))\n",
    "    triples.extend(rm_dups(abstract_triples))\n",
    "    triples.extend(rm_dups(body_triples))\n",
    "    triples.extend(rm_dups(bib_triples))\n",
    "    \n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mesh_relations(kg_data):\n",
    "    \"\"\"\n",
    "    Fill knowledge graph with supporting references from MeSH\n",
    "    \"\"\"\n",
    "    mesh_url = \"https://id.nlm.nih.gov/mesh/lookup/term?label={query}&match=contains&limit=5\"\n",
    "\n",
    "    triples = []\n",
    "    for idx, row in enumerate(kg_data.iterrows()):\n",
    "        entity = row[1]['entity']\n",
    "\n",
    "        req = requests.get(mesh_url.format(query=entity),\n",
    "                           headers={\"content-type\": \"application/json\"}, verify=False)\n",
    "\n",
    "        req_eval = eval(req.text)\n",
    "        if len(req_eval) != 0:\n",
    "            mesh_tup = [(ref['label'].lower(), row[1]['paper_id'], row[1]['relation']) for ref in req_eval]\n",
    "            triples.append(mesh_tup)\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            print(\"        {idx}/{size}\".format(idx=idx, size=kg_df.shape[0]))\n",
    "\n",
    "    mesh_df = pd.DataFrame(reduce(operator.concat, triples)) \n",
    "    mesh_df.columns = [\"entity\", \"paper_id\", \"relation\"]\n",
    "    \n",
    "    return mesh_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_knowledge_graph(paper_list):\n",
    "    \"\"\"Create Knowledge Graph\"\"\"\n",
    "    \n",
    "    # Construct KG from nlp pipeline\n",
    "    relations = list(map(functools.partial(get_relation, nlp), papers_list))\n",
    "    kg_df = pd.DataFrame(reduce(operator.concat, relations))\n",
    "    kg_df.columns = [\"entity\", \"paper_id\", \"relation\"]\n",
    "    \n",
    "    # fill in KG with connects in MESH ontology\n",
    "    full_knowledge_graph = get_mesh_relations(kg_df)\n",
    "    \n",
    "    # combine the two sets of triples\n",
    "    return pd.concat([kg_df, full_knowledge_graph],\n",
    "                     axis=0).reset_index(drop=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"en_core_web_lg\"\n",
    "nlp = spacy.load(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating stopword filter component of nlp\n",
    "com_wrds_df = pd.read_csv(\"../data/external_data/commom_words.csv\", header=0)\n",
    "for word in com_wrds_df.iterrows():\n",
    "    nlp.vocab[(word[1]['words'])].is_stop = True\n",
    "\n",
    "def remove_stopwords(doc):\n",
    "    \"\"\"Spacy Component that removes stopwords.\"\"\"\n",
    "    return [token for token in doc if not token.is_stop]\n",
    "\n",
    "nlp.add_pipe(remove_stopwords, name=\"filter_stopwords\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = list(map(functools.partial(get_relation, nlp), papers_list))\n",
    "\n",
    "kg_df = pd.DataFrame(reduce(operator.concat, relations))\n",
    "kg_df.columns = [\"entity\", \"paper_id\", \"relation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_knowledge_graph = get_mesh_relations(kg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_knowledge_graph = pd.concat([kg_df, mesh_df],axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_knowledge_graph.to_csv(\"sampleresult1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red\">BERT Embeddings</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_trf_bertbaseuncased_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U spacy[cuda92]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    pass\n",
    "    #torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = \"en_trf_bertbaseuncased_lg\"\n",
    "bert_nlp = spacy.load(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedddings(paper_list, embed_pipeline):\n",
    "        \n",
    "    def get_id_title(paper):\n",
    "        return (paper['paper_id'],\n",
    "                paper['metadata']['title'])\n",
    "    \n",
    "    def get_embed(emb_mdl, text):\n",
    "        # Summing gives us sentence level embedding\n",
    "        # https://colab.research.google.com/github/explosion/spacy-pytorch-transformers/blob/master/examples/Spacy_Transformers_Demo.ipynb\n",
    "        return (text[0], emb_mdl(text[1]).tensor.sum(axis=0))\n",
    "        \n",
    "    # processing embeddings\n",
    "    title_text = map(get_id_title, papers_list)\n",
    "    title_embeds = list(map(partial(get_embed, embed_pipeline),\n",
    "                            title_text))\n",
    "    \n",
    "    # converting to dataframe\n",
    "    embd_df = pd.DataFrame(title_embeds) # convert to dataframe\n",
    "    embd_df = pd.DataFrame(embd_df[1].values.tolist(), index = embd_df[0]) # split vector into multiple columns\n",
    "    embd_df.index = embd_df.index.set_names('paper_id')# rename index to paper_id\n",
    "    \n",
    "    return embd_df\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "embd_df = generate_embedddings(papers_list, bert_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "embd_df.to_csv(\"sandbox_output/bert_title_embeddings.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
